# Arabic-Topic-Modeling.

BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique.



  BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained natural language processing model developed by Google. It uses a deep neural network to understand and represent the meaning of text. BERT has been shown to achieve state-of-the-art results on a wide range of natural language processing tasks, including text classification, question answering, and language translation.





Requirements: 
* re
* pandas
* bertopic
* flair.embeddings  
* gensim 
* sklearn 
* numpy

